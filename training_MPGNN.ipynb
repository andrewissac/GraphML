{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import dgl\n",
    "import copy\n",
    "import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import awkward as ak\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from trainresults import TrainResults\n",
    "from train_eval_func import train, evaluate\n",
    "from copy import deepcopy\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from TauGraphDatasetInfo import TauGraphDatasetInfo\n",
    "from MPGNN import MPGNN\n",
    "from TauGraphDataset import TauGraphDataset, GetNodeFeatureVectors, GetEdgeFeatureVectors, GetNeighborNodes, GetEdgeList\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['text.usetex'] = True\n",
    "lw = 2\n",
    "xyLabelFontSize = 20\n",
    "xLabelPad = 10\n",
    "yLabelPad = 15\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_logits = []\n",
    "\n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "        edgeFeatVec = GetEdgeFeatureVectors(batched_graph)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batched_graph, nodeFeatVec, edgeFeatVec)\n",
    "\n",
    "        y_true.append(labels.detach().cpu())\n",
    "        y_logits.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_logits = torch.cat(y_logits, dim = 0)\n",
    "    y_softmax = nn.functional.softmax(y_logits, dim=1)\n",
    "    y_scoreClass1 = y_softmax[:, 1]\n",
    "    y_pred = y_logits.numpy().argmax(1)\n",
    "    \n",
    "    num_correct_pred = (y_pred == y_true).sum().item()\n",
    "    num_total_pred = len(y_true)\n",
    "    acc =  num_correct_pred / num_total_pred\n",
    "    \n",
    "    evalDict = {\n",
    "        \"y_true\": y_true.tolist(), \n",
    "        \"y_logits\": y_logits.tolist(), \n",
    "        \"y_scoreClass1\": y_scoreClass1.tolist(),\n",
    "        \"y_pred\": y_pred.tolist(), \n",
    "        \"acc\": acc\n",
    "    }\n",
    "\n",
    "    return evalDict\n",
    "\n",
    "\n",
    "def train(model, device, dataloader, optimizer, loss_fn, batchsize, results):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    epochLoss = 0.0\n",
    "    batchIter = 0\n",
    "    \n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "        edgeFeatVec = GetEdgeFeatureVectors(batched_graph)\n",
    "\n",
    "        #forward\n",
    "        pred =  model(batched_graph, nodeFeatVec, edgeFeatVec)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # multiply running loss by the number of graphs, \n",
    "        # since CrossEntropy loss calculates mean of the losses of the graphs in the batch\n",
    "        runningTotalLoss = loss.item() #* batchsize\n",
    "        results.addRunningLoss(runningTotalLoss)\n",
    "        epochLoss += runningTotalLoss\n",
    "        batchIter += 1\n",
    "        \n",
    "    return epochLoss/batchIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def trainEpochs(model, device, dataloader, optimizer, loss_fn, batchsize, nEpochs):\n",
    "    results = TrainResults()\n",
    "    bestModel = None\n",
    "    bestTestAcc = 0.0\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        # train\n",
    "        epochLoss = train(model, device, dataloader, optimizer, loss_fn, batchsize, results)\n",
    "\n",
    "        # evaluate\n",
    "        train_result = evaluate(model, device, train_dataloader)\n",
    "        test_result = evaluate(model, device, test_dataloader)\n",
    "\n",
    "        results.addEpochResult(epochLoss, train_result, test_result)\n",
    "        results.printLastResult()\n",
    "\n",
    "        if results.best_test_acc > bestTestAcc:\n",
    "            bestValAcc = results.best_test_acc\n",
    "            bestModel = copy.deepcopy(model)\n",
    "\n",
    "    return results, bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetNames(datasetDir):\n",
    "    files = glob.glob(datasetDir + '/*.json', recursive=True)\n",
    "    files.sort()\n",
    "    datasetDirectories = [path.dirname(file) for file in files]\n",
    "    datasetnames = [path.normpath(dir).split(path.sep)[-1] for dir in datasetDirectories]\n",
    "    return datasetDirectories, datasetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "['/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n",
      "['Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "datasetDir = '/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets'\n",
    "datasetDirs, datasetNames = getDatasetNames(datasetDir)\n",
    "print(datasetDirs)\n",
    "print(datasetNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetName = datasetNames[0]\n",
    "datasetDir = datasetDirs[0]\n",
    "dataset = TauGraphDataset(datasetName, datasetDir)\n",
    "dataset.printProperties()\n",
    "\n",
    "graph, label = dataset[0]\n",
    "print(graph)\n",
    "print(f'Label: {label}')\n",
    "print(GetNodeFeatureVectors(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Graphs_DYJetsToLL_M-50_genuineTaus_and_jets\n",
      "Epoch: 0, Loss: 0.3686, Train: 0.884, Test: 0.885, AUC: 0.955\n",
      "Epoch: 1, Loss: 0.2537, Train: 0.885, Test: 0.887, AUC: 0.959\n",
      "Epoch: 2, Loss: 0.2366, Train: 0.916, Test: 0.917, AUC: 0.966\n",
      "Epoch: 3, Loss: 0.2268, Train: 0.918, Test: 0.920, AUC: 0.968\n",
      "Epoch: 4, Loss: 0.2214, Train: 0.920, Test: 0.920, AUC: 0.970\n",
      "Epoch: 5, Loss: 0.2151, Train: 0.918, Test: 0.920, AUC: 0.970\n",
      "Epoch: 6, Loss: 0.2123, Train: 0.920, Test: 0.922, AUC: 0.971\n",
      "Epoch: 7, Loss: 0.2088, Train: 0.923, Test: 0.924, AUC: 0.971\n",
      "Epoch: 8, Loss: 0.2076, Train: 0.923, Test: 0.924, AUC: 0.971\n",
      "Epoch: 9, Loss: 0.2050, Train: 0.924, Test: 0.924, AUC: 0.973\n",
      "Best epoch: \n",
      "Epoch: 9, Loss: 0.2050, Train: 0.924, Test: 0.924, AUC: 0.973\n",
      "------------------(1/1) models trained------------------\n",
      "\n",
      "2173.6101315021515 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "\n",
    "batchSize = 64\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for i in range(len(datasetDirs)):\n",
    "    dataset = TauGraphDataset(datasetNames[i], datasetDirs[i])\n",
    "    splitIndices = dataset.get_split_indices()\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(splitIndices['train'])\n",
    "    test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "\n",
    "    train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=batchSize, drop_last=False)\n",
    "    test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "    \n",
    "    # Create the model with given dimensions\n",
    "    model = MPGNN(\n",
    "        dataset.dim_nfeats, dataset.dim_efeats, \n",
    "        node_out_feats=16, edge_hidden_feats=16, \n",
    "        num_step_message_passing=2, n_classes=dataset.num_graph_classes).to(device)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 10\n",
    "    \n",
    "    # train\n",
    "    print(f'Beginning training on dataset {datasetNames[i]}')\n",
    "    results, bestmodel = trainEpochs(model, device, train_dataloader, optimizer, loss_fn, batchSize, epochs)\n",
    "    results.printBestResult()\n",
    "    \n",
    "    # save results\n",
    "    outputFolder = path.join(datasetDirs[i], 'Output_MPGNN_LR0001')\n",
    "    Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results.savePlots(outputFolder)\n",
    "    results.dumpSummary(outputFolder)\n",
    "    results.pickledump(outputFolder)\n",
    "    \n",
    "    # save the best model for inference. (when loading for inference -> model.eval()!! )\n",
    "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "    torch.save(bestmodel.state_dict(), path.join(outputFolder, 'model.pt'))\n",
    "    \n",
    "    print(f'------------------({i+1}/{len(datasetDirs)}) models trained------------------\\n')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - now\n",
    "print(f'{elapsed} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e711687f31016629c64c84d23d27d9292aefc640ef90f9639fb07f22fea1b3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
