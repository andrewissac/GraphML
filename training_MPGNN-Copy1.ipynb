{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import dgl\n",
    "import copy\n",
    "import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import awkward as ak\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from trainresults import TrainResults\n",
    "from train_eval_func import train, evaluate\n",
    "from copy import deepcopy\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from TauGraphDatasetInfo import TauGraphDatasetInfo\n",
    "from MPGNN import MPGNN, MPGNN_Mean\n",
    "from TauGraphDataset import TauGraphDataset, GetNodeFeatureVectors, GetEdgeFeatureVectors, GetNeighborNodes, GetEdgeList\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['text.usetex'] = True\n",
    "lw = 2\n",
    "xyLabelFontSize = 20\n",
    "xLabelPad = 10\n",
    "yLabelPad = 15\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, dataloader, loss_fn):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_logits = []\n",
    "    epochLoss = 0.0\n",
    "    batchIter = 0\n",
    "\n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "        edgeFeatVec = GetEdgeFeatureVectors(batched_graph)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batched_graph, nodeFeatVec, edgeFeatVec)\n",
    "\n",
    "        loss = loss_fn(pred,labels)\n",
    "        epochLoss += loss.item()\n",
    "        y_true.append(labels.detach().cpu())\n",
    "        y_logits.append(pred.detach().cpu())\n",
    "        batchIter += 1\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_logits = torch.cat(y_logits, dim = 0)\n",
    "    y_softmax = nn.functional.softmax(y_logits, dim=1)\n",
    "    y_scoreClass1 = y_softmax[:, 1]\n",
    "    y_pred = y_logits.numpy().argmax(1)\n",
    "    \n",
    "    num_correct_pred = (y_pred == y_true).sum().item()\n",
    "    num_total_pred = len(y_true)\n",
    "    acc =  num_correct_pred / num_total_pred\n",
    "    \n",
    "    evalDict = {\n",
    "        \"y_true\": y_true.tolist(), \n",
    "        \"y_logits\": y_logits.tolist(), \n",
    "        \"y_scoreClass1\": y_scoreClass1.tolist(),\n",
    "        \"y_pred\": y_pred.tolist(), \n",
    "        \"acc\": acc,\n",
    "        \"loss\" : epochLoss / batchIter\n",
    "    }\n",
    "\n",
    "    return evalDict\n",
    "\n",
    "\n",
    "def train(model, device, dataloader, optimizer, loss_fn, batchsize, results):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    epochLoss = 0.0\n",
    "    batchIter = 0\n",
    "    \n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "        edgeFeatVec = GetEdgeFeatureVectors(batched_graph)\n",
    "\n",
    "        #forward\n",
    "        pred =  model(batched_graph, nodeFeatVec, edgeFeatVec)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # multiply running loss by the number of graphs, \n",
    "        # since CrossEntropy loss calculates mean of the losses of the graphs in the batch\n",
    "        runningTotalLoss = loss.item() #* batchsize\n",
    "        results.addRunningLoss(runningTotalLoss)\n",
    "        epochLoss += runningTotalLoss\n",
    "        batchIter += 1\n",
    "        \n",
    "    return epochLoss/batchIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def trainEpochs(model, device, dataloader, optimizer, loss_fn, batchsize, nEpochs):\n",
    "    results = TrainResults()\n",
    "    results.startTrainingTimer()\n",
    "    bestModel = None\n",
    "    bestValAcc = 0.0\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        # train\n",
    "        epochLoss = train(model, device, dataloader, optimizer, loss_fn, batchsize, results)\n",
    "\n",
    "        # evaluate\n",
    "        train_result = evaluate(model, device, train_dataloader, loss_fn)\n",
    "        val_result = evaluate(model, device, val_dataloader, loss_fn)\n",
    "        test_result = evaluate(model, device, test_dataloader, loss_fn)\n",
    "\n",
    "        results.addEpochResult(epochLoss, train_result, val_result, test_result)\n",
    "        results.printLastResult()\n",
    "\n",
    "        if results.best_val_acc > bestValAcc:\n",
    "            bestValAcc = results.best_val_acc\n",
    "            bestModel = copy.deepcopy(model)\n",
    "    \n",
    "    results.endTrainingTimer()\n",
    "\n",
    "    return results, bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetNames(datasetDir):\n",
    "    files = glob.glob(datasetDir + '/*.json', recursive=True)\n",
    "    files.sort()\n",
    "    datasetDirectories = [path.dirname(file) for file in files]\n",
    "    datasetnames = [path.normpath(dir).split(path.sep)[-1] for dir in datasetDirectories]\n",
    "    return datasetDirectories, datasetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "['/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n",
      "['Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "datasetDir = '/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets'\n",
    "datasetDirs, datasetNames = getDatasetNames(datasetDir)\n",
    "print(datasetDirs)\n",
    "print(datasetNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetName = datasetNames[0]\n",
    "datasetDir = datasetDirs[0]\n",
    "dataset = TauGraphDataset(datasetName, datasetDir)\n",
    "dataset.printProperties()\n",
    "\n",
    "graph, label = dataset[0]\n",
    "print(graph)\n",
    "print(f'Label: {label}')\n",
    "print(GetNodeFeatureVectors(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Graphs_DYJetsToLL_M-50_genuineTaus_and_jets\n",
      "Epoch: 0, Loss: 0.5151, Validation Loss: 0.4062, Train: 0.850, Validation: 0.852, Test: 0.855, AUC: 0.951\n",
      "Epoch: 1, Loss: 0.3804, Validation Loss: 0.3564, Train: 0.897, Validation: 0.897, Test: 0.900, AUC: 0.952\n",
      "Epoch: 2, Loss: 0.3403, Validation Loss: 0.3380, Train: 0.873, Validation: 0.873, Test: 0.876, AUC: 0.952\n",
      "Epoch: 3, Loss: 0.3153, Validation Loss: 0.2947, Train: 0.901, Validation: 0.901, Test: 0.905, AUC: 0.958\n",
      "Epoch: 4, Loss: 0.2774, Validation Loss: 0.2906, Train: 0.883, Validation: 0.882, Test: 0.883, AUC: 0.958\n",
      "Epoch: 5, Loss: 0.2692, Validation Loss: 0.2538, Train: 0.904, Validation: 0.904, Test: 0.906, AUC: 0.961\n",
      "Epoch: 6, Loss: 0.2553, Validation Loss: 0.2565, Train: 0.908, Validation: 0.907, Test: 0.911, AUC: 0.963\n",
      "Epoch: 7, Loss: 0.2487, Validation Loss: 0.2463, Train: 0.904, Validation: 0.904, Test: 0.906, AUC: 0.963\n",
      "Epoch: 8, Loss: 0.2444, Validation Loss: 0.2356, Train: 0.910, Validation: 0.910, Test: 0.912, AUC: 0.964\n",
      "Epoch: 9, Loss: 0.2437, Validation Loss: 0.2401, Train: 0.911, Validation: 0.911, Test: 0.914, AUC: 0.963\n",
      "Epoch: 10, Loss: 0.2385, Validation Loss: 0.2703, Train: 0.901, Validation: 0.901, Test: 0.904, AUC: 0.963\n",
      "Epoch: 11, Loss: 0.2358, Validation Loss: 0.2347, Train: 0.911, Validation: 0.911, Test: 0.913, AUC: 0.965\n",
      "Epoch: 12, Loss: 0.2340, Validation Loss: 0.2291, Train: 0.912, Validation: 0.912, Test: 0.915, AUC: 0.966\n",
      "Epoch: 13, Loss: 0.2324, Validation Loss: 0.2418, Train: 0.904, Validation: 0.903, Test: 0.905, AUC: 0.966\n",
      "Epoch: 14, Loss: 0.2310, Validation Loss: 0.2282, Train: 0.916, Validation: 0.916, Test: 0.919, AUC: 0.967\n",
      "Epoch: 15, Loss: 0.2321, Validation Loss: 0.2282, Train: 0.914, Validation: 0.914, Test: 0.917, AUC: 0.967\n",
      "Epoch: 16, Loss: 0.2264, Validation Loss: 0.2271, Train: 0.912, Validation: 0.911, Test: 0.914, AUC: 0.967\n",
      "Epoch: 17, Loss: 0.2260, Validation Loss: 0.2236, Train: 0.914, Validation: 0.913, Test: 0.915, AUC: 0.967\n",
      "Epoch: 18, Loss: 0.2251, Validation Loss: 0.2197, Train: 0.918, Validation: 0.918, Test: 0.919, AUC: 0.967\n",
      "Epoch: 19, Loss: 0.2218, Validation Loss: 0.2228, Train: 0.917, Validation: 0.917, Test: 0.920, AUC: 0.967\n",
      "Epoch: 20, Loss: 0.2223, Validation Loss: 0.2229, Train: 0.915, Validation: 0.914, Test: 0.916, AUC: 0.967\n",
      "Epoch: 21, Loss: 0.2229, Validation Loss: 0.2168, Train: 0.919, Validation: 0.919, Test: 0.921, AUC: 0.968\n",
      "Epoch: 22, Loss: 0.2228, Validation Loss: 0.2281, Train: 0.911, Validation: 0.909, Test: 0.911, AUC: 0.968\n",
      "Epoch: 23, Loss: 0.2209, Validation Loss: 0.2334, Train: 0.914, Validation: 0.913, Test: 0.915, AUC: 0.968\n",
      "Epoch: 24, Loss: 0.2212, Validation Loss: 0.2155, Train: 0.919, Validation: 0.918, Test: 0.922, AUC: 0.968\n",
      "Epoch: 25, Loss: 0.2183, Validation Loss: 0.2159, Train: 0.918, Validation: 0.917, Test: 0.920, AUC: 0.968\n",
      "Epoch: 26, Loss: 0.2168, Validation Loss: 0.2138, Train: 0.920, Validation: 0.920, Test: 0.922, AUC: 0.969\n",
      "Epoch: 27, Loss: 0.2197, Validation Loss: 0.2128, Train: 0.919, Validation: 0.919, Test: 0.921, AUC: 0.969\n",
      "Epoch: 28, Loss: 0.2201, Validation Loss: 0.2177, Train: 0.919, Validation: 0.917, Test: 0.920, AUC: 0.969\n",
      "Epoch: 29, Loss: 0.2164, Validation Loss: 0.2252, Train: 0.914, Validation: 0.912, Test: 0.916, AUC: 0.969\n",
      "Best epoch: \n",
      "Epoch: 26, Loss: 0.2168, Validation Loss: 0.2138, Train: 0.920, Validation: 0.920, Test: 0.922, AUC: 0.969\n",
      "\n",
      "The training took 3619 seconds (60.32 minutes).\n",
      "------------------(1/1) models trained------------------\n",
      "\n",
      "3665.161320924759 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "\n",
    "paramList = [(16,128, 2)] # node_hiddenfeats, #edge_hiddenfeats, #num_messagepasses\n",
    "            \n",
    "\n",
    "batchSize = 1024\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for n_hidden,e_hidden, msgpasses in paramList:\n",
    "    for i in range(len(datasetDirs)):\n",
    "        dataset = TauGraphDataset(datasetNames[i], datasetDirs[i])\n",
    "        splitIndices = dataset.get_split_indices()\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(splitIndices['train'])\n",
    "        val_sampler = SubsetRandomSampler(splitIndices['valid'])\n",
    "        test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "\n",
    "        train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=batchSize, drop_last=False)\n",
    "        val_dataloader = GraphDataLoader(dataset, sampler=val_sampler, batch_size=batchSize, drop_last=False)\n",
    "        test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "\n",
    "        # Create the model with given dimensions\n",
    "        model = MPGNN(\n",
    "            dataset.dim_nfeats, dataset.dim_efeats, \n",
    "            node_out_feats=n_hidden, edge_hidden_feats=e_hidden, \n",
    "            num_step_message_passing=msgpasses, n_classes=dataset.num_graph_classes).to(device)\n",
    "\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        epochs = 30\n",
    "\n",
    "        # train\n",
    "        print(f'Beginning training on dataset {datasetNames[i]}')\n",
    "        results, bestmodel = trainEpochs(model, device, train_dataloader, optimizer, loss_fn, batchSize, epochs)\n",
    "        results.printBestResult()\n",
    "\n",
    "        # save results\n",
    "        outputFolder = path.join(datasetDirs[i], f'Output_MPGNN_NHiddenFeats_{n_hidden}_EHiddenFeats_{e_hidden}_MsgPasses_{msgpasses}')\n",
    "        Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results.savePlots(outputFolder)\n",
    "        results.dumpSummary(outputFolder)\n",
    "        results.pickledump(outputFolder)\n",
    "\n",
    "        # save the best model for inference. (when loading for inference -> model.eval()!! )\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "        torch.save(bestmodel.state_dict(), path.join(outputFolder, 'model.pt'))\n",
    "\n",
    "        print(f'------------------({i+1}/{len(datasetDirs)}) models trained------------------\\n')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - now\n",
    "print(f'{elapsed} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Graphs_DYJetsToLL_M-50_genuineTaus_and_jets\n",
      "Epoch: 0, Loss: 0.5858, Validation Loss: 0.4950, Train: 0.791, Validation: 0.790, Test: 0.792, AUC: 0.873\n",
      "Epoch: 1, Loss: 0.4366, Validation Loss: 0.3924, Train: 0.832, Validation: 0.831, Test: 0.833, AUC: 0.921\n",
      "Epoch: 2, Loss: 0.3737, Validation Loss: 0.3491, Train: 0.872, Validation: 0.870, Test: 0.873, AUC: 0.938\n",
      "Epoch: 3, Loss: 0.3456, Validation Loss: 0.3324, Train: 0.872, Validation: 0.872, Test: 0.875, AUC: 0.943\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "\n",
    "paramList = [(16,64, 2),# node_hiddenfeats, #edge_hiddenfeats, #num_messagepasses\n",
    "          (16,128, 2),\n",
    "          (32, 8, 2),\n",
    "          (32, 16, 2),\n",
    "          (32, 32, 2),\n",
    "          (16, 8, 7)]\n",
    "\n",
    "batchSize = 1024\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for n_hidden,e_hidden, msgpasses in paramList:\n",
    "    for i in range(len(datasetDirs)):\n",
    "        dataset = TauGraphDataset(datasetNames[i], datasetDirs[i])\n",
    "        splitIndices = dataset.get_split_indices()\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(splitIndices['train'])\n",
    "        val_sampler = SubsetRandomSampler(splitIndices['valid'])\n",
    "        test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "\n",
    "        train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=batchSize, drop_last=False)\n",
    "        val_dataloader = GraphDataLoader(dataset, sampler=val_sampler, batch_size=batchSize, drop_last=False)\n",
    "        test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "\n",
    "        # Create the model with given dimensions\n",
    "        model = MPGNN_Mean(\n",
    "            dataset.dim_nfeats, dataset.dim_efeats, \n",
    "            node_out_feats=n_hidden, edge_hidden_feats=e_hidden, \n",
    "            num_step_message_passing=msgpasses, n_classes=dataset.num_graph_classes).to(device)\n",
    "\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        epochs = 30\n",
    "\n",
    "        # train\n",
    "        print(f'Beginning training on dataset {datasetNames[i]}')\n",
    "        results, bestmodel = trainEpochs(model, device, train_dataloader, optimizer, loss_fn, batchSize, epochs)\n",
    "        results.printBestResult()\n",
    "\n",
    "        # save results\n",
    "        outputFolder = path.join(datasetDirs[i], f'Output_MPGNN_Mean_NHiddenFeats_{n_hidden}_EHiddenFeats_{e_hidden}_MsgPasses_{msgpasses}')\n",
    "        Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results.savePlots(outputFolder)\n",
    "        results.dumpSummary(outputFolder)\n",
    "        results.pickledump(outputFolder)\n",
    "\n",
    "        # save the best model for inference. (when loading for inference -> model.eval()!! )\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "        torch.save(bestmodel.state_dict(), path.join(outputFolder, 'model.pt'))\n",
    "\n",
    "        print(f'------------------({i+1}/{len(datasetDirs)}) models trained------------------\\n')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - now\n",
    "print(f'{elapsed} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = MPGNN(\n",
    "    7, 3, \n",
    "    node_out_feats=16, edge_hidden_feats=16, \n",
    "    num_step_message_passing=2, n_classes=2)\n",
    "\n",
    "folder = '/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_MPGNN/model.pt'\n",
    "model.load_state_dict(torch.load(folder))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batchSize=64\n",
    "dataset = TauGraphDataset(datasetNames[0], datasetDirs[0])\n",
    "splitIndices = dataset.get_split_indices()\n",
    "test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "\n",
    "res = evaluate(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['y_true'][:10])\n",
    "print(res['y_logits'][:10])\n",
    "print(res['y_scoreClass1'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_logits = torch.cat(y_logits, dim = 0)\n",
    "    y_softmax = nn.functional.softmax(y_logits, dim=1)\n",
    "    y_scoreClass1 = y_softmax[:, 1]\n",
    "    y_pred = y_logits.numpy().argmax(1)\n",
    "\"\"\"\n",
    "import torch\n",
    "logits = torch.tensor(res[\"y_logits\"])\n",
    "print(\"list to tensor done\")\n",
    "\n",
    "# NN output plot\n",
    "predictions = torch.nn.functional.softmax(logits, dim=1).numpy()\n",
    "print(\"softmax done\")\n",
    "print(f'len(predictions): {len(predictions)}')\n",
    "#print(predictions)\n",
    "\n",
    "# TODO: check which order is actually signal (genuineTau) and which are background (fakeTau)\n",
    "genuineTau_decisions = predictions[:, 0]\n",
    "fakeTau_decisions = predictions[:, 1]\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.hist(genuineTau_decisions, label='Genuine Taus', \n",
    "        histtype='step', # lineplot that's unfilled\n",
    "        linewidth=lw)\n",
    "plt.hist(fakeTau_decisions, label='Jets', \n",
    "        histtype='step', # lineplot that's unfilled\n",
    "        linewidth=lw)\n",
    "plt.xlabel('Neural Network output') # add x-axis label\n",
    "plt.ylabel('Frequency') # add y-axis label\n",
    "plt.legend(loc=\"upper center\") # add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[:100, 0])\n",
    "print(predictions[:100, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e711687f31016629c64c84d23d27d9292aefc640ef90f9639fb07f22fea1b3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
