{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import dgl\n",
    "import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from trainresults import TrainResults\n",
    "from train_eval_func import train, evaluate\n",
    "from copy import deepcopy\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from TauGraphDatasetInfo import TauGraphDatasetInfo\n",
    "from TauGraphDataset import TauGraphDataset, GetNodeFeatureVectors, GetEdgeFeatureVectors\n",
    "from TauGraphDataset import GetNeighborNodes, GetEdgeList, GetEdgeFeatureVectorsFromSourceNode, Graph2FlatZeropaddedList\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['text.usetex'] = True\n",
    "lw = 2\n",
    "xyLabelFontSize = 20\n",
    "xLabelPad = 10\n",
    "yLabelPad = 15\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetNames(datasetDir):\n",
    "    files = glob.glob(datasetDir + '/*.json', recursive=True)\n",
    "    files.sort()\n",
    "    datasetDirectories = [path.dirname(file) for file in files]\n",
    "    datasetnames = [path.normpath(dir).split(path.sep)[-1] for dir in datasetDirectories]\n",
    "    return datasetDirectories, datasetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n",
      "['Graphs_DYJetsToLL_M-50_genuineTaus_and_jets']\n"
     ]
    }
   ],
   "source": [
    "datasetDir = '/ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets'\n",
    "datasetDirs, datasetNames = getDatasetNames(datasetDir)\n",
    "print(datasetDirs)\n",
    "print(datasetNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n",
      "<TauGraphDataset.TauGraphDataset object at 0x7fb3291c5b38>\n"
     ]
    }
   ],
   "source": [
    "dataset = TauGraphDataset(datasetNames[0], datasetDirs[0])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Graphs_DYJetsToLL_M-50_genuineTaus_and_jets,\n",
      " directory: /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets\n",
      "Graph(num_nodes=25, num_edges=600,\n",
      "      ndata_schemes={'feat': Scheme(shape=(7,), dtype=torch.float64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.float32)})\n",
      "label: 1\n",
      "graph classes: ['0', '1']\n",
      "dataset graph count: 200000\n",
      "nodeFeatKeys: ['pt', 'eta', 'phi', 'mass', 'charge', 'particleType', 'summand']\n",
      "edgeFeatKeys: ['deltaEta', 'deltaPhi', 'deltaR']\n",
      "graphFeatkeys: ['nodeCount', 'tau_byDeepTau2017v2p1VSjet']\n",
      "max node count: 81\n",
      "min node count: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'name: {datasetNames[0]},\\n directory: {datasetDirs[0]}')\n",
    "graph, label = dataset[0]\n",
    "print(graph)\n",
    "print(f'label: {label}')\n",
    "print(f'graph classes: {dataset.graphClasses}')\n",
    "print(f'dataset graph count: {dataset.num_graphs}')\n",
    "print(f'nodeFeatKeys: {dataset.nodeFeatKeys}')\n",
    "print(f'edgeFeatKeys: {dataset.edgeFeatKeys}')\n",
    "print(f'graphFeatkeys: {dataset.graphFeatKeys}')\n",
    "print(f'max node count: {dataset.maxNodeCount}')\n",
    "print(f'min node count: {dataset.minNodeCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=25, num_edges=600,\n",
      "      ndata_schemes={'feat': Scheme(shape=(7,), dtype=torch.float64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.float32)})\n",
      "nFeatDim: 7\n",
      "eFeatDim: 3\n",
      "maxNodeCount: 81\n",
      "\n",
      "node + edge features dim per Node (includes zero padding if nodecount<maxnodecount):\n",
      " nFeatDim + eFeatDim * (maxNodeCount-1) = 7 + 3 * 80 =  247\n",
      "The (maxNodeCount-1) comes from fully connected graphs without self-loops\n",
      "\n",
      "only node features dim per Node: nFeatDim=7\n",
      "\n",
      "node + edge features with zero padding to fill until maxNodeCount leads to inputsize: 20007\n",
      "check example: node + edge features list size: 20007\n",
      "\n",
      "only node features with zero padding to fill until maxNodeCount leads to inputsize: 567\n",
      "check example: only node features list size: 567\n"
     ]
    }
   ],
   "source": [
    "graphs, labels = dataset[:]\n",
    "g = graphs[0]\n",
    "nFeatDim = dataset.dim_nfeats\n",
    "eFeatDim = dataset.dim_efeats\n",
    "maxNodeCount = dataset.maxNodeCount\n",
    "print(g)\n",
    "print(f'nFeatDim: {nFeatDim}')\n",
    "print(f'eFeatDim: {eFeatDim}')\n",
    "print(f'maxNodeCount: {maxNodeCount}')\n",
    "print()\n",
    "\n",
    "nodeAndEdgeFeaturePaddedDim = nFeatDim + eFeatDim * (maxNodeCount - 1)\n",
    "print(f'node + edge features dim per Node (includes zero padding if nodecount<maxnodecount):\\n',\n",
    "      f'nFeatDim + eFeatDim * (maxNodeCount-1) = {nFeatDim} + {eFeatDim} * {maxNodeCount - 1} = ',\n",
    "      f'{nodeAndEdgeFeaturePaddedDim}')\n",
    "print(f'The (maxNodeCount-1) comes from fully connected graphs without self-loops')\n",
    "print()\n",
    "print(f'only node features dim per Node: nFeatDim={nFeatDim}')\n",
    "print()\n",
    "\n",
    "nodeAndEdgeFeaturePaddedDimInputSize = nodeAndEdgeFeaturePaddedDim * maxNodeCount\n",
    "nodeFeaturePaddedDimInputSize = nFeatDim * maxNodeCount\n",
    "print(f'node + edge features with zero padding to fill until maxNodeCount leads to inputsize: {nodeAndEdgeFeaturePaddedDimInputSize}')\n",
    "useEdgeFeat = True\n",
    "temp = np.array(Graph2FlatZeropaddedList(g, nFeatDim, eFeatDim, maxNodeCount, useEdgeFeat), dtype=np.float32)\n",
    "print(f'check example: node + edge features list size: {len(temp)}')\n",
    "print()\n",
    "print(f'only node features with zero padding to fill until maxNodeCount leads to inputsize: {nodeFeaturePaddedDimInputSize}')\n",
    "useEdgeFeat = False\n",
    "temp = np.array(Graph2FlatZeropaddedList(g, nFeatDim, eFeatDim, maxNodeCount, useEdgeFeat), dtype=np.float32)\n",
    "print(f'check example: only node features list size: {len(temp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputData(dgldataset, useEdgeFeatures):\n",
    "    inputs = [] \n",
    "    graphs, labels = dgldataset[:]\n",
    "    maxNodeCount = dgldataset.maxNodeCount\n",
    "    nFeatDim = dgldataset.dim_nfeats\n",
    "    eFeatDim = dgldataset.dim_efeats\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    it = 0\n",
    "    \n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        inputs.append(Graph2FlatZeropaddedList(graphs[i], nFeatDim, eFeatDim, maxNodeCount, useEdgeFeatures))\n",
    "\n",
    "    # Stack all inputs_ vertically\n",
    "    print(type(inputs))\n",
    "    inputs = np.array(inputs, dtype=np.float32)\n",
    "    print(\"before vstack - Input shape: \", inputs.shape)\n",
    "    print(inputs)\n",
    "    print(type(inputs))\n",
    "    inputs = np.vstack(inputs)\n",
    "    print(inputs)\n",
    "    \n",
    "\n",
    "    # Stack all labels_ horizontally\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    print(\"Input shape: \", inputs.shape)\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    print(labels.shape)\n",
    "    print(labels[0])\n",
    "    end = time.time() - start\n",
    "    print(f'graphs to flattened zero padded list took {end:.2f} seconds ({end/60:.2f} minutes)')\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [14:22<00:00, 231.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "before vstack - Input shape:  (200000, 20007)\n",
      "[[ 0.6791992   1.238197   -0.6319463  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.7011719  -0.5762505   2.2737963  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 4.2695312  -0.781518    0.42511293 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.66308594 -0.8608051   1.7206644  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.4853516   1.7232581  -3.10292    ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.8852539   1.0530717  -2.2728148  ...  0.          0.\n",
      "   0.        ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.6791992   1.238197   -0.6319463  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.7011719  -0.5762505   2.2737963  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 4.2695312  -0.781518    0.42511293 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.66308594 -0.8608051   1.7206644  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.4853516   1.7232581  -3.10292    ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.8852539   1.0530717  -2.2728148  ...  0.          0.\n",
      "   0.        ]]\n",
      "Input shape:  (200000, 20007)\n",
      "Labels shape:  (200000,)\n",
      "(200000, 2)\n",
      "[0. 1.]\n",
      "graphs to flattened zero padded list took 882.52 seconds (14.71 minutes)\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = getInputData(dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagenerator(inputs, labels, batchsize):\n",
    "    while True:\n",
    "        start = 0\n",
    "        end = batchsize\n",
    "\n",
    "        while start  < len(inputs): \n",
    "            # load your images from numpy arrays or read from directory\n",
    "            x = inputs[start:end] \n",
    "            y = labels[start:end]\n",
    "            yield x, y\n",
    "\n",
    "            start += batchsize\n",
    "            end += batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"KerasModel_NodeAndEdgeFeat\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               (None, 32)                640256    \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 652,850\n",
      "Trainable params: 652,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"KerasModel_NodeAndEdgeFeat\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               (None, 32)                640256    \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 652,850\n",
      "Trainable params: 652,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "outputFolder = path.join(datasetDir, 'Output_Keras_NodeAndEdgeFeat')\n",
    "Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = keras.Sequential(name=\"KerasModel_NodeAndEdgeFeat\")\n",
    "inputDim = (nFeatDim + eFeatDim * (maxNodeCount - 1)) * maxNodeCount\n",
    "model.add(keras.layers.InputLayer(input_shape=(inputDim,), name=\"input\"))\n",
    "model.add(keras.layers.Dense(32, activation='relu', name=\"dense1\"))\n",
    "model.add(keras.layers.Dense(16*16, activation='relu', name=\"dense2\"))\n",
    "model.add(keras.layers.Dense(16, activation='relu', name=\"dense3\"))\n",
    "model.add(keras.layers.Dense(2, activation='softmax', name=\"output\"))\n",
    "model.summary()\n",
    "\n",
    "lossfunction = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "#earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0.0005)\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=path.join(outputFolder,'keras_nodeAndEdgeFeat_bestmodel.h5'), monitor='val_loss', save_best_only=True, verbose=1)\n",
    "csvlogger = tf.keras.callbacks.CSVLogger(filename=path.join(outputFolder, 'results_keras_nodeAndEdgeFeat_bestmodel.csv'), separator=',', append=False)\n",
    "#callbacks = [earlystopping, modelcheckpoint, csvlogger]\n",
    "callbacks = [modelcheckpoint, csvlogger]\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer, loss=lossfunction, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 140000\n",
      "validation samples: 40200\n",
      "test samples: 19800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_testAndVal, y_train, y_testAndVal = train_test_split(inputs, labels, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_testAndVal, y_testAndVal, test_size=0.33, shuffle=False)\n",
    "print(f'train samples: {len(X_train)}')\n",
    "print(f'validation samples: {len(X_val)}')\n",
    "print(f'test samples: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "136/136 [==============================] - 30s 206ms/step - loss: 0.2776 - accuracy: 0.8933 - val_loss: 0.2480 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24803, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 0.2426 - accuracy: 0.9057 - val_loss: 0.2414 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24803 to 0.24141, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 0.2363 - accuracy: 0.9077 - val_loss: 0.2369 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24141 to 0.23695, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 0.2314 - accuracy: 0.9093 - val_loss: 0.2359 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23695 to 0.23589, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 0.2269 - accuracy: 0.9111 - val_loss: 0.2316 - val_accuracy: 0.9088\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23589 to 0.23165, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 0.2228 - accuracy: 0.9127 - val_loss: 0.2291 - val_accuracy: 0.9097\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23165 to 0.22914, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 0.2188 - accuracy: 0.9144 - val_loss: 0.2284 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22914 to 0.22841, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 29s 217ms/step - loss: 0.2159 - accuracy: 0.9154 - val_loss: 0.2283 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22841 to 0.22828, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 0.2129 - accuracy: 0.9165 - val_loss: 0.2282 - val_accuracy: 0.9104\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22828 to 0.22820, saving model to /ceph/aissac/ntuple_for_graphs/prod_2018_v2_processed_v5_THESIS/trimmed_200000_and_cut_puppiWeightNoLep_greater_0_and_deltaR_smaller_0point5/Graphs_DYJetsToLL_M-50_genuineTaus_and_jets/Output_Keras_NodeAndEdgeFeat/keras_nodeAndEdgeFeat_bestmodel.h5\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 29s 218ms/step - loss: 0.2102 - accuracy: 0.9176 - val_loss: 0.2285 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22820\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 29s 218ms/step - loss: 0.2074 - accuracy: 0.9190 - val_loss: 0.2288 - val_accuracy: 0.9103\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22820\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 0.2053 - accuracy: 0.9202 - val_loss: 0.2290 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22820\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 0.2033 - accuracy: 0.9212 - val_loss: 0.2303 - val_accuracy: 0.9093\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22820\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 0.2012 - accuracy: 0.9220 - val_loss: 0.2313 - val_accuracy: 0.9092\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22820\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 25s 188ms/step - loss: 0.1992 - accuracy: 0.9227 - val_loss: 0.2323 - val_accuracy: 0.9093\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22820\n",
      "Epoch 16/30\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 0.1973 - accuracy: 0.9236 - val_loss: 0.2339 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22820\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 0.1958 - accuracy: 0.9241 - val_loss: 0.2350 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.22820\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 29s 214ms/step - loss: 0.1938 - accuracy: 0.9248 - val_loss: 0.2363 - val_accuracy: 0.9095\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22820\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 29s 216ms/step - loss: 0.1923 - accuracy: 0.9258 - val_loss: 0.2366 - val_accuracy: 0.9094\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22820\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 29s 218ms/step - loss: 0.1905 - accuracy: 0.9264 - val_loss: 0.2381 - val_accuracy: 0.9095\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22820\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 30s 219ms/step - loss: 0.1891 - accuracy: 0.9269 - val_loss: 0.2397 - val_accuracy: 0.9088\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.22820\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 0.1877 - accuracy: 0.9275 - val_loss: 0.2411 - val_accuracy: 0.9088\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22820\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 29s 218ms/step - loss: 0.1856 - accuracy: 0.9284 - val_loss: 0.2434 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.22820\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 0.1842 - accuracy: 0.9288 - val_loss: 0.2446 - val_accuracy: 0.9081\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22820\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 0.1831 - accuracy: 0.9291 - val_loss: 0.2469 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.22820\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 0.1821 - accuracy: 0.9301 - val_loss: 0.2490 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.22820\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 0.1808 - accuracy: 0.9300 - val_loss: 0.2517 - val_accuracy: 0.9079\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.22820\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 26s 196ms/step - loss: 0.1801 - accuracy: 0.9305 - val_loss: 0.2533 - val_accuracy: 0.9069\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.22820\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 16s 117ms/step - loss: 0.1788 - accuracy: 0.9312 - val_loss: 0.2558 - val_accuracy: 0.9059\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.22820\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 14s 100ms/step - loss: 0.1778 - accuracy: 0.9314 - val_loss: 0.2583 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.22820\n",
      "training took 807.56 seconds (13.46 minutes)\n"
     ]
    }
   ],
   "source": [
    "batchsize=1024\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    x = datagenerator(X_train, y_train, batchsize=batchsize),\n",
    "    validation_data = datagenerator(X_val, y_val,batchsize=batchsize),\n",
    "    steps_per_epoch = len(X_train)//batchsize,\n",
    "    validation_steps = len(X_val)//batchsize,\n",
    "    shuffle=False, # at creation from rootfiles -> graphs already shuffled dataset\n",
    "    epochs = 30, # doesnt matter, since we use early stopping\n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "end = time.time() - start\n",
    "print(f'training took {end:.2f} seconds ({end/60:.2f} minutes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "{'loss': [0.27760863304138184, 0.24260340631008148, 0.236345574259758, 0.23141776025295258, 0.22689875960350037, 0.2227974236011505, 0.2188325673341751, 0.21586543321609497, 0.2128785252571106, 0.21016502380371094, 0.20741058886051178, 0.20530423521995544, 0.20333971083164215, 0.20123834908008575, 0.1992451697587967, 0.19728560745716095, 0.19575738906860352, 0.19383575022220612, 0.19230349361896515, 0.1905279904603958, 0.18913604319095612, 0.18766145408153534, 0.18558406829833984, 0.1842328906059265, 0.18309332430362701, 0.18209059536457062, 0.1808345913887024, 0.18014033138751984, 0.1788051873445511, 0.17777903378009796], 'accuracy': [0.8932530879974365, 0.9056671857833862, 0.9076747298240662, 0.9093225002288818, 0.9111069440841675, 0.912704348564148, 0.9143809080123901, 0.9154170751571655, 0.916517972946167, 0.9176045060157776, 0.9190219640731812, 0.9202020764350891, 0.921159029006958, 0.9219721555709839, 0.9226557016372681, 0.9236055016517639, 0.9241092205047607, 0.9248287677764893, 0.9258432984352112, 0.926354169845581, 0.9269010424613953, 0.9275414347648621, 0.9283761382102966, 0.9288150668144226, 0.9290741086006165, 0.9301246404647827, 0.9300382733345032, 0.9305132031440735, 0.9311967492103577, 0.9313766360282898], 'val_loss': [0.2480325698852539, 0.24141082167625427, 0.23694530129432678, 0.2358868420124054, 0.2316490262746811, 0.2291417419910431, 0.22841358184814453, 0.22828170657157898, 0.228204146027565, 0.22854359447956085, 0.22877314686775208, 0.22900299727916718, 0.23025131225585938, 0.23126186430454254, 0.23233866691589355, 0.23386052250862122, 0.2350490242242813, 0.2363213449716568, 0.2366274744272232, 0.23807665705680847, 0.23973751068115234, 0.24109004437923431, 0.24344363808631897, 0.24462105333805084, 0.24685955047607422, 0.24900147318840027, 0.2517161965370178, 0.2532956302165985, 0.25580519437789917, 0.2583187222480774], 'val_accuracy': [0.9037710428237915, 0.9061498641967773, 0.9074268937110901, 0.9083784222602844, 0.9088040590286255, 0.9096554517745972, 0.9105067849159241, 0.910081148147583, 0.9104316830635071, 0.9101812839508057, 0.9102563858032227, 0.910081148147583, 0.9093049168586731, 0.9092047214508057, 0.9092798233032227, 0.9095553159713745, 0.9100561141967773, 0.9094801545143127, 0.9093800187110901, 0.9094551205635071, 0.9088291525840759, 0.9087790250778198, 0.9084284901618958, 0.9080528616905212, 0.9074268937110901, 0.9067758321762085, 0.9078776240348816, 0.9068509340286255, 0.9059495329856873, 0.9060747027397156]}\n",
      "4375/4375 [==============================] - 12s 3ms/step - loss: 0.1855 - accuracy: 0.9285\n",
      "619/619 [==============================] - 2s 3ms/step - loss: 0.2539 - accuracy: 0.9114\n",
      "1257/1257 [==============================] - 4s 3ms/step - loss: 0.2582 - accuracy: 0.9063\n",
      "Train: 0.928, Valid: 0.906, Test: 0.911, AUC: 0.962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 648x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NN output plot\n",
    "predictions = model.predict(X_test)\n",
    "print(type(predictions))\n",
    "#print(predictions)\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "lw = 2\n",
    "xyLabelFontSize = 20\n",
    "xLabelPad = 10\n",
    "yLabelPad = 15\n",
    "\n",
    "def createFigure():\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    ax.tick_params(pad=7)\n",
    "    return fig, ax\n",
    "\n",
    "# TODO: check which order is actually signal (genuineTau) and which are background (fakeTau)\n",
    "genuineTau_decisions = predictions[:,0]\n",
    "fakeTau_decisions = predictions[:,1]\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.hist(genuineTau_decisions, label='Genuine Taus', \n",
    "        histtype='step', # lineplot that's unfilled\n",
    "        density=True, # normalize to form a probability density\n",
    "        linewidth=lw)\n",
    "plt.hist(fakeTau_decisions, label='Jets', \n",
    "        histtype='step', # lineplot that's unfilled\n",
    "        density=True, linewidth=lw) # normalize to form a probability density\n",
    "plt.xlabel('Neural Network output') # add x-axis label\n",
    "plt.ylabel('Arbitrary units') # add y-axis label\n",
    "plt.legend(loc=\"upper center\") # add legend\n",
    "plt.savefig(path.join(outputFolder, \"NN_output.png\"))\n",
    "plt.clf()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# most tutorials slice the prediction for whatever reason with [:,1] but why?\n",
    "# predictions_ = predictions[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test.argmax(axis=1), predictions[:, 1])\n",
    "roc_auc = auc(fpr, tpr) # area under curve (AUC), ROC = Receiver operating characteristic\n",
    "\n",
    "fig, ax = createFigure()\n",
    "ax.plot(fpr, tpr, label=f'ROC (area = {roc_auc:.2f})', linewidth=lw)\n",
    "ax.plot([0, 1], [0, 1], '--', color='red', label='Luck', linewidth=lw)\n",
    "ax.set_xlabel('False Positive Rate') \n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "outputFilePath = path.join(outputFolder, 'ROC.png')\n",
    "plt.savefig(outputFilePath)\n",
    "plt.clf()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(history.history)\n",
    "\n",
    "# Plot accuracy of NN\n",
    "fig, ax = createFigure()\n",
    "ax.plot(history.history['accuracy'], label='Training', linewidth=lw)\n",
    "ax.plot(history.history['val_accuracy'], label='Validation', linewidth=lw)\n",
    "ax.set_ylabel('Accuracy', labelpad=xLabelPad, fontsize=xyLabelFontSize)\n",
    "ax.set_xlabel('Epoch', labelpad=yLabelPad, fontsize=xyLabelFontSize)\n",
    "ax.legend()\n",
    "outputFilePath = path.join(outputFolder, 'accuracy.png')\n",
    "plt.savefig(outputFilePath)\n",
    "plt.clf()\n",
    "# Plot loss of NN\n",
    "fig, ax = createFigure()\n",
    "ax.plot(history.history['loss'], label=\"Training\",linewidth=lw)\n",
    "ax.plot(history.history['val_loss'], label=\"Validation\", linewidth=lw)\n",
    "ax.set_ylabel('Loss', labelpad=xLabelPad, fontsize=xyLabelFontSize)\n",
    "ax.set_xlabel('Epoch', labelpad=yLabelPad, fontsize=xyLabelFontSize)\n",
    "ax.legend()\n",
    "outputFilePath = path.join(outputFolder, 'epochloss.png')\n",
    "plt.savefig(outputFilePath)\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=1)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "_, valid_acc = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f'Train: {train_acc:.3f}, Valid: {valid_acc:.3f}, Test: {test_acc:.3f}, AUC: {roc_auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e711687f31016629c64c84d23d27d9292aefc640ef90f9639fb07f22fea1b3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
